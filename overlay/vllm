#!/usr/bin/env bash
set -euo pipefail

HOST="${HOST:-0.0.0.0}"
PORT="${PORT:-8000}"

# vllm-stack calls: vllm serve <model> --host ... --port ... --gpu_memory_utilization ...
if [[ $# -ge 1 && "$1" == "serve" ]]; then
  shift
  if [[ $# -ge 1 ]]; then
    MODEL_ARG="$1"
    shift
    # Use chart's model arg unless MODEL_NAME already set
    export MODEL_NAME="${MODEL_NAME:-$MODEL_ARG}"
  fi
fi

while [[ $# -gt 0 ]]; do
  case "$1" in
    --host) HOST="$2"; shift 2 ;;
    --port) PORT="$2"; shift 2 ;;

    --gpu_memory_utilization|--gpu-memory-utilization)
      export GPU_MEMORY_UTILIZATION="$2"; shift 2 ;;

    --max_model_len|--max-model-len)
      export MAX_MODEL_LEN="$2"; shift 2 ;;

    --max_num_seqs|--max-num-seqs)
      export MAX_NUM_SEQS="$2"; shift 2 ;;

    --tensor_parallel_size|--tensor-parallel-size)
      export TENSOR_PARALLEL_SIZE="$2"; shift 2 ;;

    --dtype)
      export VLLM_DTYPE="$2"; shift 2 ;;

    --served-model-name)
      export SERVED_MODEL_NAME="$2"; shift 2 ;;

    *)
      shift ;;
  esac
done

export HOST PORT
exec python /app/entrypoint.py
